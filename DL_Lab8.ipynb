{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e70a712-975a-434f-9509-c513d471cb8c",
   "metadata": {},
   "source": [
    "# Deep Learning Lab Experiment - 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fffbc2-fdf3-4184-b229-42455c68b974",
   "metadata": {},
   "source": [
    "# 1- Task: Given a sequence of alphabets (with some missing values), use an RNN and a Bidirectional RNN model to predict the missing values in the sequence.\n",
    "Steps: \n",
    "1. Create the dataset consisting of a sequence of alphabets. \n",
    "2. Preprocess the data by encoding the alphabet characters and handling missing values. \n",
    "3. Build and train an RNN model for sequence prediction. \n",
    "4. Build and train a Bidirectional RNN model for comparison. \n",
    "5. Predict the missing values using both models.\n",
    "   \n",
    "E.g. :  M A C H I N __ predict E  And using Bidirectional RNN - A C H I N E. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02316f22-d290-416b-8aba-6c989b9eea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\Users\\shubh\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 733ms/step\n",
      "Original sequence: M A C H I N _\n",
      "Complete sequence: M A C H I N E\n",
      "RNN prediction: _\n",
      "Bidirectional RNN prediction: _\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\n",
      "Reverse sequence: _ A C H I N E\n",
      "RNN prediction: N\n",
      "Bidirectional RNN prediction: N\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import string\n",
    "\n",
    "# 1. Create and prepare the dataset\n",
    "def create_dataset():\n",
    "    # Using alphabet sequence as base\n",
    "    alphabet = list(string.ascii_uppercase)\n",
    "    \n",
    "    # Example sequence with missing values\n",
    "    sequence = \"M A C H I N _\".split()  # \"_\" represents missing value\n",
    "    complete_sequence = \"M A C H I N E\".split()  # Ground truth\n",
    "    \n",
    "    return sequence, complete_sequence, alphabet\n",
    "\n",
    "# 2. Preprocess the data\n",
    "def preprocess_data(sequence, alphabet):\n",
    "    # Create character to index mapping\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(alphabet + ['_'])}\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    \n",
    "    # Convert sequence to numerical form\n",
    "    X = np.array([char_to_idx[char] for char in sequence])\n",
    "    \n",
    "    # Create input-output pairs (shifted by 1)\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    sequence_length = 3  # Look at 3 characters to predict next\n",
    "    \n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_data.append(X[i:i + sequence_length])\n",
    "        y_data.append(X[i + sequence_length])\n",
    "    \n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "    \n",
    "    # Reshape for RNN [samples, timesteps, features]\n",
    "    X_data = X_data.reshape((X_data.shape[0], X_data.shape[1], 1))\n",
    "    \n",
    "    return X_data, y_data, char_to_idx, idx_to_char\n",
    "\n",
    "# 3. Build and train RNN model\n",
    "def build_rnn_model(vocab_size, sequence_length):\n",
    "    model = keras.Sequential([\n",
    "        layers.SimpleRNN(64, input_shape=(sequence_length, 1), return_sequences=False),\n",
    "        layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 4. Build and train Bidirectional RNN model\n",
    "def build_birnn_model(vocab_size, sequence_length):\n",
    "    model = keras.Sequential([\n",
    "        layers.Bidirectional(layers.SimpleRNN(64, return_sequences=False), \n",
    "                           input_shape=(sequence_length, 1)),\n",
    "        layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 5. Predict missing values\n",
    "def predict_missing(model, sequence, char_to_idx, idx_to_char, sequence_length):\n",
    "    # Prepare input for prediction\n",
    "    input_seq = sequence[-sequence_length:]\n",
    "    X_pred = np.array([char_to_idx[char] for char in input_seq])\n",
    "    X_pred = X_pred.reshape(1, sequence_length, 1)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(X_pred)\n",
    "    predicted_idx = np.argmax(prediction)\n",
    "    return idx_to_char[predicted_idx]\n",
    "\n",
    "def main():\n",
    "    # Create dataset\n",
    "    sequence, complete_sequence, alphabet = create_dataset()\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_data, y_data, char_to_idx, idx_to_char = preprocess_data(sequence, alphabet)\n",
    "    vocab_size = len(char_to_idx)\n",
    "    \n",
    "    # Build and train RNN model\n",
    "    sequence_length = 3\n",
    "    rnn_model = build_rnn_model(vocab_size, sequence_length)\n",
    "    rnn_model.fit(X_data, y_data, epochs=100, verbose=0)\n",
    "    \n",
    "    # Build and train Bidirectional RNN model\n",
    "    birnn_model = build_birnn_model(vocab_size, sequence_length)\n",
    "    birnn_model.fit(X_data, y_data, epochs=100, verbose=0)\n",
    "    \n",
    "    # Predict using both models\n",
    "    rnn_prediction = predict_missing(rnn_model, sequence, char_to_idx, idx_to_char, sequence_length)\n",
    "    birnn_prediction = predict_missing(birnn_model, sequence, char_to_idx, idx_to_char, sequence_length)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Original sequence: {' '.join(sequence)}\")\n",
    "    print(f\"Complete sequence: {' '.join(complete_sequence)}\")\n",
    "    print(f\"RNN prediction: {rnn_prediction}\")\n",
    "    print(f\"Bidirectional RNN prediction: {birnn_prediction}\")\n",
    "    \n",
    "    # For reverse example \"_ A C H I N E\"\n",
    "    reverse_sequence = \"_ A C H I N E\".split()\n",
    "    X_data_rev, y_data_rev, _, _ = preprocess_data(reverse_sequence, alphabet)\n",
    "    \n",
    "    # Retrain models for reverse sequence\n",
    "    rnn_model.fit(X_data_rev, y_data_rev, epochs=100, verbose=0)\n",
    "    birnn_model.fit(X_data_rev, y_data_rev, epochs=100, verbose=0)\n",
    "    \n",
    "    reverse_rnn_pred = predict_missing(rnn_model, reverse_sequence, char_to_idx, idx_to_char, sequence_length)\n",
    "    reverse_birnn_pred = predict_missing(birnn_model, reverse_sequence, char_to_idx, idx_to_char, sequence_length)\n",
    "    \n",
    "    print(f\"\\nReverse sequence: {' '.join(reverse_sequence)}\")\n",
    "    print(f\"RNN prediction: {reverse_rnn_pred}\")\n",
    "    print(f\"Bidirectional RNN prediction: {reverse_birnn_pred}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a70ab-5ff7-468c-ae1f-b403a2966df6",
   "metadata": {},
   "source": [
    "# 2- Predict the next word in a sentence using an RNN. Consider the following sentence \n",
    "Dataset: \n",
    "The cat sat on the mat. \n",
    "The dog sat on the rug. \n",
    "The bird flew in the sky. \n",
    "The cat jumped over the fence. \n",
    "And predict “The cat sat on __-“ \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b4baec5-4d98-47a5-a048-7b23ab87c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Text Preprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Define the dataset\n",
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog sat on the rug\",\n",
    "    \"The bird flew in the sky\",\n",
    "    \"The cat jumped over the fence\"\n",
    "]\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1  # +1 for padding/indexing\n",
    "\n",
    "# Generate input sequences\n",
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_len = max(len(x) for x in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e520626c-e590-485e-b8a5-87d96d7b8d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,800</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">975</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m10\u001b[0m)               │             \u001b[38;5;34m150\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_3 (\u001b[38;5;33mSimpleRNN\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,800\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                  │             \u001b[38;5;34m975\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,925</span> (23.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,925\u001b[0m (23.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,925</span> (23.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,925\u001b[0m (23.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Model Building\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=10))  # Removed input_length\n",
    "model.add(SimpleRNN(64))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_seq_len - 1))  # Explicitly build the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7561215-b50e-4e1e-a7ff-3b33b635c537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22fc307dfa0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Training the Model\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35b245b2-cb26-434a-8ee6-b69c2fb8c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sat on the\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Prediction\n",
    "def predict_next_word(seed_text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_index = np.argmax(predicted)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_index:\n",
    "            return word\n",
    "\n",
    "seed_text = \"The cat sat on\"\n",
    "predicted_word = predict_next_word(seed_text, max_seq_len)\n",
    "print(f\"{seed_text} {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974a72b-eee1-4adb-a950-0eadae43d2ed",
   "metadata": {},
   "source": [
    "# 3- Develop a sequence generator for Indian Classical Music Raga using an RNN to predict the next note in a series. The notes involved are Sa, Re, Ga, Ma, Pa, Dha, Ni, and Sha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e0ae7c24-ffd6-4ff6-b67b-a56886545944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "\n",
    "# Step 1: Dataset Preparation\n",
    "def create_raga_sequences():\n",
    "    # Basic notes (Swaras)\n",
    "    notes = ['Sa', 'Re', 'Ga', 'Ma', 'Pa', 'Dha', 'Ni', 'Sha']\n",
    "    \n",
    "    # Define some raga scales (simplified versions)\n",
    "    raga_scales = {\n",
    "        'Bhairav': ['Sa', 'Re', 'Ga', 'Ma', 'Pa', 'Dha', 'Ni', 'Sha'],  # Full scale for simplicity\n",
    "        'Bhopali': ['Sa', 'Re', 'Ga', 'Pa', 'Dha', 'Sa'],  # Arohana scale\n",
    "        'Bageshree': ['Sa', 'Ga', 'Ma', 'Dha', 'Ni', 'Sa'],  # Simplified Arohana\n",
    "    }\n",
    "    \n",
    "    # Note to integer mapping\n",
    "    note_to_int = {note: i for i, note in enumerate(notes)}\n",
    "    int_to_note = {i: note for i, note in enumerate(notes)}\n",
    "    \n",
    "    # Generate sequences\n",
    "    sequences = []\n",
    "    for raga, scale in raga_scales.items():\n",
    "        for _ in range(20):  # Generate 20 sequences per raga\n",
    "            seq_length = random.randint(5, 10)\n",
    "            sequence = [random.choice(scale) for _ in range(seq_length)]\n",
    "            sequences.append(sequence)\n",
    "    \n",
    "    return sequences, note_to_int, int_to_note, notes, raga_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b6876834-6acb-4cb1-a6af-9a70f5fa0735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess Data\n",
    "def preprocess_data(sequences, note_to_int, seq_length=10):\n",
    "    X, y = [], []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        # Convert notes to integers\n",
    "        num_seq = [note_to_int[note] for note in seq]\n",
    "        # Create input-output pairs\n",
    "        for i in range(len(num_seq) - 1):\n",
    "            X.append(num_seq[:i + 1])\n",
    "            y.append(num_seq[i + 1])\n",
    "    \n",
    "    # Pad sequences\n",
    "    X = keras.utils.pad_sequences(X, maxlen=seq_length, padding='pre')\n",
    "    y = keras.utils.to_categorical(y, num_classes=len(note_to_int))\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d92319d4-d14d-4131-b00a-e519ee55cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build RNN Model\n",
    "def build_rnn_model(vocab_size=8):\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(vocab_size, 64),\n",
    "        layers.LSTM(128, return_sequences=False),  # Using LSTM instead of SimpleRNN for better memory\n",
    "        layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2115fbbe-f38e-4024-b807-2dd89cb36b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN Model...\n",
      "Epoch 1/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.1705 - loss: 2.0603 - val_accuracy: 0.0800 - val_loss: 2.0787\n",
      "Epoch 2/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2046 - loss: 1.9896 - val_accuracy: 0.3333 - val_loss: 2.1022\n",
      "Epoch 3/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2265 - loss: 1.9529 - val_accuracy: 0.4133 - val_loss: 2.0124\n",
      "Epoch 4/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.1848 - loss: 1.9708 - val_accuracy: 0.4133 - val_loss: 2.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2420 - loss: 1.9272 - val_accuracy: 0.4133 - val_loss: 2.0230\n",
      "Epoch 6/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2013 - loss: 1.9379 - val_accuracy: 0.1600 - val_loss: 2.0358\n",
      "Epoch 7/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.2292 - loss: 1.9524 - val_accuracy: 0.3733 - val_loss: 1.9542\n",
      "Epoch 8/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.2685 - loss: 1.9021 - val_accuracy: 0.3600 - val_loss: 2.0146\n",
      "Epoch 9/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.2494 - loss: 1.8917 - val_accuracy: 0.3600 - val_loss: 1.9553\n",
      "Epoch 10/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.2111 - loss: 1.8880 - val_accuracy: 0.3733 - val_loss: 1.9171\n",
      "Epoch 11/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2900 - loss: 1.8396 - val_accuracy: 0.4133 - val_loss: 1.9114\n",
      "Epoch 12/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2490 - loss: 1.8514 - val_accuracy: 0.3333 - val_loss: 1.8934\n",
      "Epoch 13/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2741 - loss: 1.8282 - val_accuracy: 0.3467 - val_loss: 1.8509\n",
      "Epoch 14/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.2644 - loss: 1.8298 - val_accuracy: 0.3333 - val_loss: 1.9017\n",
      "Epoch 15/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.2819 - loss: 1.8458 - val_accuracy: 0.4000 - val_loss: 1.8997\n",
      "Epoch 16/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.2656 - loss: 1.8421 - val_accuracy: 0.3333 - val_loss: 1.9308\n",
      "Epoch 17/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2595 - loss: 1.8013 - val_accuracy: 0.3467 - val_loss: 1.8910\n",
      "Epoch 18/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2548 - loss: 1.8292 - val_accuracy: 0.4000 - val_loss: 1.9042\n",
      "Epoch 19/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.2736 - loss: 1.8175 - val_accuracy: 0.3467 - val_loss: 1.8752\n",
      "Epoch 20/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.2836 - loss: 1.7888 - val_accuracy: 0.3200 - val_loss: 1.9253\n",
      "Epoch 21/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.2728 - loss: 1.8211 - val_accuracy: 0.4000 - val_loss: 1.8811\n",
      "Epoch 22/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3361 - loss: 1.7318 - val_accuracy: 0.4000 - val_loss: 1.8524\n",
      "Epoch 23/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3296 - loss: 1.7369 - val_accuracy: 0.2800 - val_loss: 2.0067\n",
      "Epoch 24/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2832 - loss: 1.7706 - val_accuracy: 0.2800 - val_loss: 1.9185\n",
      "Epoch 25/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3259 - loss: 1.7560 - val_accuracy: 0.4133 - val_loss: 1.9218\n",
      "Epoch 26/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3395 - loss: 1.7206 - val_accuracy: 0.2933 - val_loss: 1.9556\n",
      "Epoch 27/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.2899 - loss: 1.7420 - val_accuracy: 0.3867 - val_loss: 1.8829\n",
      "Epoch 28/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.3258 - loss: 1.7103 - val_accuracy: 0.4000 - val_loss: 1.9530\n",
      "Epoch 29/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.3017 - loss: 1.7224 - val_accuracy: 0.2933 - val_loss: 1.9903\n",
      "Epoch 30/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3051 - loss: 1.7265 - val_accuracy: 0.4000 - val_loss: 1.9832\n",
      "Epoch 31/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3679 - loss: 1.7053 - val_accuracy: 0.2800 - val_loss: 2.0911\n",
      "Epoch 32/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.3017 - loss: 1.6929 - val_accuracy: 0.3733 - val_loss: 1.9513\n",
      "Epoch 33/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.3417 - loss: 1.6755 - val_accuracy: 0.3867 - val_loss: 1.9808\n",
      "Epoch 34/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.3664 - loss: 1.6581 - val_accuracy: 0.3200 - val_loss: 2.1333\n",
      "Epoch 35/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.3290 - loss: 1.6875 - val_accuracy: 0.4000 - val_loss: 2.0111\n",
      "Epoch 36/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3632 - loss: 1.6619 - val_accuracy: 0.3200 - val_loss: 1.9889\n",
      "Epoch 37/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.3333 - loss: 1.6737 - val_accuracy: 0.3333 - val_loss: 2.1231\n",
      "Epoch 38/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3648 - loss: 1.5985 - val_accuracy: 0.2933 - val_loss: 2.0882\n",
      "Epoch 39/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.3952 - loss: 1.6041 - val_accuracy: 0.2933 - val_loss: 1.9930\n",
      "Epoch 40/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.3858 - loss: 1.5775 - val_accuracy: 0.3200 - val_loss: 2.2147\n",
      "Epoch 41/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3626 - loss: 1.6061 - val_accuracy: 0.4000 - val_loss: 2.1003\n",
      "Epoch 42/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.4329 - loss: 1.5712 - val_accuracy: 0.4000 - val_loss: 2.0466\n",
      "Epoch 43/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.3773 - loss: 1.6196 - val_accuracy: 0.2933 - val_loss: 2.2291\n",
      "Epoch 44/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.3896 - loss: 1.5339 - val_accuracy: 0.2667 - val_loss: 2.2193\n",
      "Epoch 45/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.3775 - loss: 1.5778 - val_accuracy: 0.3867 - val_loss: 2.1745\n",
      "Epoch 46/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3883 - loss: 1.5489 - val_accuracy: 0.3200 - val_loss: 2.2539\n",
      "Epoch 47/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4265 - loss: 1.5213 - val_accuracy: 0.3733 - val_loss: 2.2818\n",
      "Epoch 48/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.4468 - loss: 1.5023 - val_accuracy: 0.3867 - val_loss: 2.2747\n",
      "Epoch 49/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.3999 - loss: 1.4933 - val_accuracy: 0.3067 - val_loss: 2.3557\n",
      "Epoch 50/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.4346 - loss: 1.4778 - val_accuracy: 0.4000 - val_loss: 2.3063\n",
      "\n",
      "Generated Raga Sequences:\n",
      "\n",
      "Raga Bhairav:\n",
      "Seed: Sa Re Ga\n",
      "Generated: Sa Re Ga Re Pa Re Re Re Sha Pa Pa Pa Re\n",
      "Generated (temp=0.8): Sa Re Ga Pa Sa Dha Dha Sa Sa Ma Dha Dha Ga\n",
      "\n",
      "Raga Bhopali:\n",
      "Seed: Sa Re Ga\n",
      "Generated: Sa Re Ga Pa Re Pa Pa Pa Dha Ga Re Dha Sa\n",
      "Generated (temp=0.8): Sa Re Ga Sa Re Sa Sa Dha Pa Ga Sa Pa Re\n",
      "\n",
      "Raga Bageshree:\n",
      "Seed: Sa Ga Ma\n",
      "Generated: Sa Ga Ma Ma Sa Ga Dha Sa Dha Dha Dha Ma Ni\n",
      "Generated (temp=0.8): Sa Ga Ma Dha Ga Sa Sa Sa Sa Ga Sa Ga Sa\n"
     ]
    }
   ],
   "source": [
    "# Step 4 & 5: Train and Generate Sequences\n",
    "def generate_sequence(model, seed_sequence, note_to_int, int_to_note, length=10, temperature=1.0):\n",
    "    generated = seed_sequence.copy()\n",
    "    num_seq = [note_to_int[note] for note in seed_sequence]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        padded_seq = keras.utils.pad_sequences([num_seq], maxlen=10, padding='pre')\n",
    "        prediction = model.predict(padded_seq, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature to predictions\n",
    "        prediction = np.log(prediction + 1e-7) / temperature\n",
    "        exp_preds = np.exp(prediction)\n",
    "        prediction = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        # Sample next note\n",
    "        next_note_idx = np.random.choice(len(prediction), p=prediction)\n",
    "        next_note = int_to_note[next_note_idx]\n",
    "        generated.append(next_note)\n",
    "        num_seq.append(next_note_idx)\n",
    "        num_seq = num_seq[1:]  # Slide window\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Prepare data\n",
    "    sequences, note_to_int, int_to_note, notes, raga_scales = create_raga_sequences()\n",
    "    X, y = preprocess_data(sequences, note_to_int)\n",
    "    \n",
    "    # Split data\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "    \n",
    "    # Build and train model\n",
    "    model = build_rnn_model()\n",
    "    print(\"Training RNN Model...\")\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "    \n",
    "    # Generate sequences for different ragas\n",
    "    print(\"\\nGenerated Raga Sequences:\")\n",
    "    for raga, scale in raga_scales.items():\n",
    "        seed = scale[:3]  # Use first 3 notes as seed\n",
    "        generated = generate_sequence(model, seed, note_to_int, int_to_note, length=10)\n",
    "        print(f\"\\nRaga {raga}:\")\n",
    "        print(\"Seed:\", ' '.join(seed))\n",
    "        print(\"Generated:\", ' '.join(generated))\n",
    "        \n",
    "        # Generate another variation with different temperature\n",
    "        generated_temp = generate_sequence(model, seed, note_to_int, int_to_note, length=10, temperature=0.8)\n",
    "        print(\"Generated (temp=0.8):\", ' '.join(generated_temp))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af2695-71e1-4c37-821c-a88057a3eb28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning Lab Experiment - 7**"
      ],
      "metadata": {
        "id": "J7LMgssHoylj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\"Image Classification Using CIFAR-10 Dataset using simple deep network with 4 hidden layers and 3 dropout layer also apply pruning and quantization to reduce size and report¬†size¬†of¬†model\"\n",
        "\n",
        "1.Train the original model on CIFAR-10.\n",
        "\n",
        "2.Save the original model (model.h5).\n",
        "\n",
        "3.Apply pruning manually:\n",
        "\n",
        "‚óè If a weight is less than 0.01, we set it to 0.\n",
        "Save the pruned model (pruned_model.h5).\n",
        "Apply post-training quantization:\n",
        "\n",
        "‚óè Converts weights from 32-bit float ‚Üí 8-bit int.\n",
        "Save the quantized model (quantized_model.tflite).\n",
        "Compare and print the sizes of all three models."
      ],
      "metadata": {
        "id": "s4GrqjqHohdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot  # For pruning and quantization\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# -------------------------------\n",
        "# 1Ô∏è‚É£ Load and Preprocess CIFAR-10 Dataset\n",
        "# -------------------------------\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values (0 to 1)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# -------------------------------\n",
        "# 2Ô∏è‚É£ Build a Simple Deep Neural Network (DNN)\n",
        "# -------------------------------\n",
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')  # 10 classes (CIFAR-10)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "original_model = create_model()\n",
        "original_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# -------------------------------\n",
        "# 3Ô∏è‚É£ Train the Original Model\n",
        "# -------------------------------\n",
        "original_model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test), batch_size=64)\n",
        "\n",
        "# Save original model and get size\n",
        "original_model.save('original_model.h5')\n",
        "original_model_size = os.path.getsize('original_model.h5') / (1024 * 1024)  # Convert to MB\n",
        "print(f\"Original Model Size: {original_model_size:.2f} MB\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4Ô∏è‚É£ Apply Pruning to Reduce Parameters\n",
        "# -------------------------------\n",
        "pruning_params = {\n",
        "    \"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=0.20, final_sparsity=0.80, begin_step=0, end_step=1000\n",
        "    )\n",
        "}\n",
        "\n",
        "# Apply pruning wrapper\n",
        "pruned_model = tf.keras.Sequential([\n",
        "    tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Flatten(input_shape=(32, 32, 3)), **pruning_params),\n",
        "    tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Dense(512, activation='relu'), **pruning_params),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Dense(256, activation='relu'), **pruning_params),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Dense(128, activation='relu'), **pruning_params),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Dense(64, activation='relu'), **pruning_params),\n",
        "    tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Dense(10, activation='softmax'), **pruning_params)\n",
        "])\n",
        "\n",
        "# Compile pruned model\n",
        "pruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the pruned model with pruning callback\n",
        "pruned_model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    validation_data=(x_test, y_test),\n",
        "    batch_size=64,\n",
        "    callbacks=[tfmot.sparsity.keras.UpdatePruningStep()]\n",
        ")\n",
        "\n",
        "# Remove pruning wrappers before saving\n",
        "stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "stripped_pruned_model.save('pruned_model.h5')\n",
        "\n",
        "# Get pruned model size\n",
        "pruned_model_size = os.path.getsize('pruned_model.h5') / (1024 * 1024)\n",
        "print(f\"Pruned Model Size: {pruned_model_size:.2f} MB\")\n",
        "\n",
        "# -------------------------------\n",
        "# 5Ô∏è‚É£ Apply Quantization for Further Compression\n",
        "# -------------------------------\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(stripped_pruned_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Apply full quantization\n",
        "\n",
        "quantized_model = converter.convert()\n",
        "\n",
        "# Save quantized model\n",
        "quantized_model_path = 'quantized_model.tflite'\n",
        "with open(quantized_model_path, 'wb') as f:\n",
        "    f.write(quantized_model)\n",
        "\n",
        "# Get quantized model size\n",
        "quantized_model_size = os.path.getsize(quantized_model_path) / (1024 * 1024)\n",
        "print(f\"Quantized Model Size: {quantized_model_size:.2f} MB\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6Ô∏è‚É£ Compare Model Sizes\n",
        "# -------------------------------\n",
        "print(\"\\nModel Size Comparison:\")\n",
        "print(f\"üîπ Original Model: {original_model_size:.2f} MB\")\n",
        "print(f\"üîπ Pruned Model (After Stripping): {pruned_model_size:.2f} MB\")\n",
        "print(f\"üîπ Quantized Model: {quantized_model_size:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Irlb_591XRm",
        "outputId": "c2268494-bbb9-4657-f974-7631b4b8d3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 19s 23ms/step - loss: 2.0247 - accuracy: 0.2445 - val_loss: 1.8585 - val_accuracy: 0.3206\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 17s 21ms/step - loss: 1.8847 - accuracy: 0.3026 - val_loss: 1.7959 - val_accuracy: 0.3528\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 17s 21ms/step - loss: 1.8349 - accuracy: 0.3298 - val_loss: 1.7475 - val_accuracy: 0.3693\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 18s 24ms/step - loss: 1.7936 - accuracy: 0.3465 - val_loss: 1.7370 - val_accuracy: 0.3718\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 17s 21ms/step - loss: 1.7598 - accuracy: 0.3596 - val_loss: 1.6727 - val_accuracy: 0.4077\n",
            "Original Model Size: 20.04 MB\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 23s 27ms/step - loss: 2.0115 - accuracy: 0.2503 - val_loss: 1.8188 - val_accuracy: 0.3449\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 1.8260 - accuracy: 0.3343 - val_loss: 1.7256 - val_accuracy: 0.3787\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 1.7711 - accuracy: 0.3573 - val_loss: 1.6658 - val_accuracy: 0.4065\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 1.7301 - accuracy: 0.3758 - val_loss: 1.6426 - val_accuracy: 0.4084\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 18s 24ms/step - loss: 1.6951 - accuracy: 0.3892 - val_loss: 1.6118 - val_accuracy: 0.4249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Model Size: 6.69 MB\n",
            "Quantized Model Size: 1.67 MB\n",
            "\n",
            "Model Size Comparison:\n",
            "üîπ Original Model: 20.04 MB\n",
            "üîπ Pruned Model (After Stripping): 6.69 MB\n",
            "üîπ Quantized Model: 1.67 MB\n"
          ]
        }
      ]
    }
  ]
}